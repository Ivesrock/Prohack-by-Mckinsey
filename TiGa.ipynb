{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eHMZpZzj32ox"
   },
   "source": [
    "## TiGa - ProHack Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rd0B0RVg4nBN"
   },
   "source": [
    "## Aproach\n",
    "1. Define the target for prediction - Y (index)\n",
    "2. Import Libraries\n",
    "3. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvUL1q_45EN0"
   },
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy9a0o6t9eaW"
   },
   "outputs": [],
   "source": [
    "# for working with dataframes import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-yQZdIq58gN"
   },
   "source": [
    "### Step 2: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "SaZgrCae3hk9",
    "outputId": "97d2e3f4-18e3-49f4-a5fd-d3dbaf3889f8"
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_4PYu6A36OM"
   },
   "source": [
    "### Step 3: Analyze & Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "g_DOMVfb3827",
    "outputId": "5a50c1e1-e336-4f6a-ce9b-c415570f0bf4"
   },
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "id": "EUtT_7I843Es",
    "outputId": "406328e4-1145-4b10-d6d5-a9eb2c3d49e6"
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "As8V9UPi7Aay"
   },
   "source": [
    "#### Evaluate for missing data\n",
    "\n",
    "The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "kplt-roH3rGB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_data = train_df.isnull()\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Count missing values in each column</h4>\n",
    "<p>\n",
    "Using a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value, \"False\"  means the value is present in the dataset.  In the body of the for loop the method  \".value_counts()\"  counts the number of \"True\" values. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"deal_missing_values\">Deal with missing data</h3>\n",
    "<b>How to deal with missing data?</b>\n",
    "\n",
    "<ol>\n",
    "    <li>drop data<br>\n",
    "        a. drop the whole row<br>\n",
    "        b. drop the whole column\n",
    "    </li>\n",
    "    <li>replace data<br>\n",
    "        a. replace it by mean<br>\n",
    "        b. replace it by frequency<br>\n",
    "        c. replace it based on other functions\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<i> !!! Whole columns should be dropped only if most entries in the column are empty. !!! </i>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping data\n",
    "Drop data columns based on the relativity to the well-being index. The selection mostly follows the OECD approach with their index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Replace by mean</b>\n",
    "<ul>\n",
    "    <li> existence expectancy index\n",
    "    <li> existence expectancy at birth\n",
    "    <li> Gross income per capita\n",
    "    <li> Income Index\n",
    "    <li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the average\n",
    "avg_exist_exp_i = train_df[\"existence expectancy index\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of existence expectancy index:\", avg_exist_exp_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_exist_exp_at_b = train_df[\"existence expectancy at birth\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of existence expectancy at birth:\", avg_exist_exp_at_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gross income per capita\n",
    "\n",
    "avg_gross_inc_pc = train_df[\"Gross income per capita\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of Gross income per capita:\", avg_gross_inc_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income Index\n",
    "\n",
    "avg_income_i = train_df[\"Income Index\"].astype(\"float\").mean(axis=0)\n",
    "print(\"Average of Income Index:\", avg_income_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing data by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"existence expectancy index\"].replace(np.nan, avg_exist_exp_i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"existence expectancy at birth\"].replace(np.nan, avg_exist_exp_at_b, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gross income per capita\n",
    "train_df[\"Gross income per capita\"].replace(np.nan, avg_gross_inc_pc, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income Index\n",
    "train_df[\"Income Index\"].replace(np.nan, avg_income_i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "missing_data = train_df.isnull()\n",
    "\n",
    "print (missing_data[\"existence expectancy index\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=train_df.drop(train_df.index[363:902])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=train1.drop(train1.index[543:1263])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3=train2.drop(train2.index[722:1441])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4=train3.drop(train3.index[902:1351])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after rows dropping here is the shape of an updated dataframe\n",
    "\n",
    "train4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_data1 = train4.isnull()\n",
    "\n",
    "for column in missing_data1.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data1[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gal=set(train4[\"galaxy\"])\n",
    "s=0\n",
    "for x in train_gal:\n",
    "    s=s+len(train4.loc[train4['galaxy'] == x])\n",
    "print(\"Total distinct galaxies: {}\".format(len(train_gal)))\n",
    "print(\"Average samples per galaxy: {}\".format(s/len(train_gal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for Cross-validating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach by Ömer Gözüaçık\n",
    "\n",
    "I trained a model for exery distinct galaxy in the training set (180) except the one from 126th galaxy as it has only one sample.\n",
    "\n",
    "I used features with top x correlation with respect to y (target variable) galaxy specific. (x is found by trying different values [20,25,30,40,50,60,70])\n",
    "\n",
    "Missing values are filled with the galaxy specific 'mean' of the data. (Median can be used alternatively.)\n",
    "\n",
    "Train and test sets are not mixed for both imputation and standardization.\n",
    "\n",
    "Standard Scaler is used to standardize data.\n",
    "\n",
    "Gradient Boosted Regression is used as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(data,cor):\n",
    "    labels= data['y']\n",
    "    data=data.drop('galaxy', axis=1)    \n",
    "    data=data.drop('y', axis=1)\n",
    "    \n",
    "    correlation=abs(data.corrwith(labels))\n",
    "    columns=correlation.nlargest(cor).index\n",
    "    data=data[columns]\n",
    "    \n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean').fit(data)\n",
    "    data=imp.transform(data)\n",
    "\n",
    "    scaler = StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "        \n",
    "    estimator = GradientBoostingRegressor(n_estimators=300)\n",
    "    \n",
    "    cv_results = cross_validate(estimator, data, labels, cv=4, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "    error=np.mean(cv_results['test_score'])\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for cross-validating a model for every galaxy.\n",
    "\n",
    "I return the mean of the cross-validation scores disregarding the differences of their sample sizes.\n",
    "Remove the lonely galaxy, occuring only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gal=set(train4[\"galaxy\"])\n",
    "train_gal.remove(\"NGC 5253\")\n",
    "def loop_train(cor):\n",
    "    errors=[]\n",
    "    for gal in train_gal:\n",
    "        index = train4.index[train4['galaxy'] == gal]\n",
    "        data = train4.loc[index]\n",
    "        errors.append(cross_validation_loop(data,cor))\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking which correlation threshold gives better value\n",
    "The model performs best when the threshold is 20 with RMSE of 0.0063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor=[20,25,30,40,50,60,70,80]\n",
    "errors=[]\n",
    "for x in cor: \n",
    "    errors.append(loop_train(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on the test data\n",
    "\n",
    "- Similar methodology is used to fill the missing value and standardization.\n",
    "- The best covariance threshold in the cross validation, 20, is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(data, test_data):\n",
    "    labels= data['y']\n",
    "    data=data.drop('galaxy', axis=1)    \n",
    "    data=data.drop('y', axis=1)\n",
    "    correlation=abs(data.corrwith(labels))\n",
    "    columns=correlation.nlargest(20).index\n",
    "    \n",
    "    train_labels= labels\n",
    "    train_data=data[columns]\n",
    "    test_data= test_data[columns]\n",
    "    \n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean').fit(train_data)\n",
    "    train_data=imp.transform(train_data)\n",
    "    test_data=imp.transform(test_data)\n",
    "\n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    model = GradientBoostingRegressor(n_estimators=300)\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predictions = model.predict(test_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting samples with respect to their unique galaxy type. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test_res\n",
    "test=test.sort_values(by=['galaxy'])\n",
    "test_pred = pd.DataFrame(0, index=np.arange(len(test)), columns=[\"predicted_y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping over all galaxy types in the test set and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for gal in test_gal:\n",
    "    count=len(test.loc[test['galaxy'] == gal])\n",
    "    index = train.index[train['galaxy'] == gal]\n",
    "    data = train.loc[index]\n",
    "    pred=test_loop(data,test.loc[test['galaxy']==gal])\n",
    "    test_pred.loc[i:i+count-1,'predicted_y'] = pred\n",
    "    i=i+count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting samples with respect to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"predicted_y\"]=test_pred.to_numpy()\n",
    "test.sort_index(inplace=True)\n",
    "predictions = test[\"predicted_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion 1\n",
    "\n",
    "- With this approach, we are **not using 8 galaxies in the training set as they are not in the test set.** (Almost 160 samples)\n",
    "\n",
    "- A better approach should use them as well.\n",
    "\n",
    "- According to our theory, every galaxy represent a country and samples are its properties at a time (maybe galactic year represents time).\n",
    "\n",
    "- Some countries may have missing values as they may have joined IBRD late. This may be organizers decision as well. Filling missing values with regression can improve performance.\n",
    "\n",
    "- World Bank categorizes countries by both region and income: https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups\n",
    "\n",
    "7 regions: East Asia and Pacific, Europe and Central Asia, Latin America & the Caribbean, Middle East and North Africa, North America, South Asia, Sub-Saharan Africa\n",
    "\n",
    "4 income groups: Low-income economies, Lower-middle-income economies, Upper-middle-income economies, High-income economies \n",
    "\n",
    "- Clustering galaxies may excel the performance of the model. I would try both clustering galaxies to either 4 or 7 clusters. Then try making imputation/training with respect to every cluster.\n",
    "\n",
    "This code is a summary of what we have done. We also analyzed RMSE for cross-validation for per galaxy. \n",
    "\n",
    "Galaxies: {128, 2, 4, 5, 133, 11, 140, 147, 153, 154, 34, 35, 40, 43, 55, 64, 76, 78, 83, 100, 101, 102, 107, 108, 119} have RMSE over 0.008. \n",
    "\n",
    "The list gives them in order, 128th having 0.008559 and 119th having 0.034926. \n",
    "\n",
    "- Fine tuning these problematic galaxies with low cross-validation scores can excel the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization part\n",
    "\n",
    "- Ideally giving 100 to top 500 samples with highest p^2 values should optimize the likely increase.\n",
    "- However, as the predictions can be faulty, this approach would result with lower Leaderboard Score.\n",
    "\n",
    "E.g: If the original p^2 value is higher than the predicted p^2, it will increase the error as we are directly giving it 0.\n",
    "\n",
    "- That's why, I believe its better to spread the risk for the samples in the bordering regions (400< [rank of p^2] <600).\n",
    "- I assign 100 energy to top 400 samples and 50 energy to the remaining top 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = predictions\n",
    "pot_inc = -np.log(index+0.01)+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2= pot_inc**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.DataFrame({\n",
    "    'Index':test.index,\n",
    "    'pred': predictions,\n",
    "    'opt_pred':0,\n",
    "    'eei':test['existence expectancy index'], # So we can split into low and high EEI galaxies\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.loc[p2.nlargest(400).index, 'opt_pred']=100\n",
    "ss=ss.sort_values('pred')\n",
    "ss.iloc[400:600].opt_pred = 50\n",
    "ss=ss.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase = (ss['opt_pred']*p2)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(increase), ss.loc[ss.eei < 0.7, 'opt_pred'].sum(), ss['opt_pred'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[['Index', 'pred', 'opt_pred']].to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion 2\n",
    "\n",
    "- Optimization can be done better by changing the spreading the risk part (assigning energy to the 400<p^2<600 region).\n",
    "\n",
    "- You can give values that are decreasing starting from 400th to 600th (99, 98, 97...). \n",
    "\n",
    "It is less likely for the 400th sample to be out of top 500, and similarly it is less likely for the 600th sample to be in the top 500. That's why, you can give more energy to the ones in the 400-500 region and less to the 500-600.\n",
    "\n",
    "- This approach got me and my friend to the best score of 0.04271993 which is ranked 22nd right now.\n",
    "\n",
    "- As we are out of top 20 and reached the upload limit, we are sharing our approach publicly to help other teams that have worse results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
